{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNFs+0NcuW7HFZv9IdZrRBY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RiccardoRubini93/SWA_DQ/blob/main/Upload_data_to_BQ_latest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76kDLASjEmSv"
      },
      "outputs": [],
      "source": [
        "from google.api_core import page_iterator\n",
        "from google.cloud import storage\n",
        "import os\n",
        "import pandas as pd \n",
        "from google.cloud import bigquery"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "print('Authenticated')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsSPjTu_Z8R7",
        "outputId": "98f9ec30-f403-44d2-ec71-6a31867eeef2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authenticated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_datetime_to_string(data):\n",
        "  return data.strftime('%Y-%d-%m | %H:%M:%S')"
      ],
      "metadata": {
        "id": "ZTYcv0jWrOl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#next step is to add the extra columns\n",
        "\n",
        "def bucket_metadata(bucket_name,project_id):\n",
        "    \"\"\"Prints out a bucket's metadata.\"\"\"\n",
        "  \n",
        "    storage_client = storage.Client(project_id)\n",
        "    bucket = storage_client.get_bucket(bucket_name)\n",
        "\n",
        "    #print(f\"ID: {bucket.id}\")\n",
        "    #print(f\"Name: {bucket.name}\")\n",
        "    #print(f\"Storage Class: {bucket.storage_class}\")\n",
        "    #print(f\"Location: {bucket.location}\")\n",
        "    #print(f\"Location Type: {bucket.location_type}\")\n",
        "\n",
        "    data                  = {}\n",
        "    data['bucket_id']     = bucket.id\n",
        "    data['bucket_name']          = bucket.name\n",
        "    data['bucket_storage_class'] = bucket.storage_class\n",
        "    data['bucket_location']      = bucket.location\n",
        "    data['bucket_location_type'] = bucket.location_type\n",
        "    #json_data = json.dumps(data)\n",
        "\n",
        "    return data\n",
        "\n",
        "def file_path_policy_respected_check(file_path):\n",
        "    #function that checks whether the path policy of a certain bucket is respected or not\n",
        "    #each path should be in the following format \n",
        "    path_chuncks = file_path.split('/')[2:-1]\n",
        "    print(path_chuncks)\n",
        "    if len(path_chuncks) == 5 : policy_respected = True\n",
        "    else : policy_respected = False\n",
        "\n",
        "    return policy_respected\n",
        "\n",
        "def _item_to_value(iterator, item):\n",
        "    return item\n",
        "\n",
        "def list_directories(bucket_name, prefix):\n",
        "\n",
        "    if prefix and not prefix.endswith('/'):\n",
        "        prefix += '/'\n",
        "\n",
        "    extra_params = {\n",
        "        \"projection\": \"noAcl\",\n",
        "        \"prefix\": prefix,\n",
        "        \"delimiter\": '/'\n",
        "    }\n",
        "\n",
        "    gcs = storage.Client()\n",
        "\n",
        "    path = \"/b/\" + bucket_name + \"/o\"\n",
        "\n",
        "    iterator = page_iterator.HTTPIterator(\n",
        "        client=gcs,\n",
        "        api_request=gcs._connection.api_request,\n",
        "        path=path,\n",
        "        items_key='prefixes',\n",
        "        item_to_value=_item_to_value,\n",
        "        extra_params=extra_params,\n",
        "    )\n",
        "\n",
        "    return [x for x in iterator]"
      ],
      "metadata": {
        "id": "WuemepCphvDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#definitions of helpers functions\n",
        "\n",
        "def file_path_pipeline_code_distinct_versions_check(input_path):\n",
        "\n",
        "  bucket = input_path.split('/')[2]\n",
        "  prefix  = input_path.split(bucket)[1][1:-4]\n",
        "\n",
        "  #build a list containing path and corresponding sizes\n",
        "\n",
        "  versions_sizes = []\n",
        "\n",
        "  for directory in list_directories(bucket,prefix):\n",
        "\n",
        "    path = \"gs://\"+ bucket + \"/\" +  directory \n",
        "    print(path)\n",
        "    size = os.popen(\"gsutil du -s \" + str(path)).read().split()\n",
        "    print(\"Size : \" + str(size))\n",
        "\n",
        "    versions_sizes.append(size)\n",
        "  return versions_sizes\n",
        "\n",
        "def get_file_number_in_landing_zone_path(input_path):\n",
        "\n",
        "    project_id = 'advanced-analytics-278408'\n",
        "    client = storage.Client(project_id)\n",
        "\n",
        "    bucket = input_path.split('/')[2]\n",
        "    prefix  = input_path.split(bucket)[1][1:-1]\n",
        "\n",
        "    count = 0\n",
        "\n",
        "    for blob in client.list_blobs(bucket,prefix=prefix):\n",
        "      count +=1\n",
        "\n",
        "    print(\"Landing zone path : \" + str(input_path) )\n",
        "    print(\"File number : \" + str(count))\n",
        "\n",
        "    return count\n",
        "\n",
        "def get_oldest_and_newest_file(input_path,project_id):\n",
        "\n",
        "  client = storage.Client(project_id)\n",
        "\n",
        "  bucket_name = input_path.split('/')[2]\n",
        "  prefix =      input_path.split(bucket)[1][1:-1]\n",
        "\n",
        "  #generate a list of blobs\n",
        "\n",
        "  blobs = [(blob, blob.updated) for blob in client.list_blobs(bucket_name,prefix = prefix,)]\n",
        "\n",
        "  #sort blobs by update date\n",
        "  newest  = sorted(blobs, key=lambda tup: tup[1])[-1] #ok funziona\n",
        "  oldest  = sorted(blobs, key=lambda tup: tup[1])[0]\n",
        "\n",
        "  return oldest,newest\n",
        "\n",
        "def is_file_policy_respected(input_path,project_id):\n",
        "\n",
        "  client = storage.Client(project_id)\n",
        "\n",
        "  name_is_conformal = True\n",
        "\n",
        "  print(\"Check that the policy of the file is respected\")\n",
        "  print(\"Bucket : \" + str(bucket))\n",
        "  print(\"Prefix : \" + str(prefix))\n",
        "\n",
        "  for blob in client.list_blobs(bucket,prefix=prefix):\n",
        "\n",
        "    if len((blob.name.split('/')[-1]).split('__')) != 4 : \n",
        "      print(\"Found not conformal file\")\n",
        "      print(blob.name)\n",
        "      name_is_conformal = False\n",
        "      break\n",
        "  \n",
        "  return name_is_conformal\n",
        "\n",
        "def extract_file_info(input_path,project_id):\n",
        "\n",
        "  client = storage.Client(project_id)\n",
        "\n",
        "  bucket_name = input_path.split('/')[2]\n",
        "  prefix =      input_path.split(bucket)[1][1:-1]\n",
        "\n",
        "  out = {}\n",
        "\n",
        "  cnt = 0\n",
        "\n",
        "  for blob in client.list_blobs(bucket,prefix=prefix):\n",
        "    \n",
        "    out['file_name_pipeline_code_info'] = blob.name.split('/')[2]\n",
        "    out['file_name_pipeline_name_info'] = blob.name.split('/')[1]\n",
        "    try: out['file_name_pipeline_windowframe_info'] = blob.name.split('/')[5].split('__')[2]\n",
        "    except: out['file_name_pipeline_windowframe_info'] = 'Missing File'\n",
        "\n",
        "    if cnt > 0 : break\n",
        "\n",
        "    cnt = +1\n",
        "\n",
        "  return out"
      ],
      "metadata": {
        "id": "XwD7H4xvQxVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#generate full list of days for the period covered \n",
        "\n",
        "from datetime import datetime, date, timedelta\n",
        "\n",
        "def generate_list(period_covered,load_time_frame):\n",
        "\n",
        "  year_s = int(period_covered[0][0:4])\n",
        "  year_e = int(period_covered[1][0:4])\n",
        "\n",
        "  month_s = int(period_covered[0][4:6])\n",
        "  month_e = int(period_covered[1][4:6])\n",
        "\n",
        "  if load_time_frame == 'dd':\n",
        "\n",
        "    day_s = int(period_covered[0][6:8])\n",
        "    day_e = int(period_covered[1][6:8])\n",
        "    freq = 'd'\n",
        "\n",
        "  elif load_time_frame == 'mm':\n",
        "\n",
        "    day_s =  1\n",
        "    day_e =  1\n",
        "    freq = 'm'\n",
        "\n",
        "  start = date(year_s, month_s, day_s)  \n",
        "  end   = date(year_e, month_e, day_e) \n",
        "  \n",
        "  time_list = []\n",
        "\n",
        "  if load_time_frame == 'mm':\n",
        "    \n",
        "    for item in pd.date_range(start,end,freq=freq).strftime('%Y%m%d').to_list():\n",
        "\n",
        "      time_list.append(item[0:6])\n",
        "  \n",
        "  elif load_time_frame == 'dd':\n",
        "\n",
        "    time_list = pd.date_range(start,end,freq=freq).strftime('%Y%m%d').to_list() \n",
        "\n",
        "  \n",
        "  return time_list\n",
        "\n",
        "#select the oldest and newest file -> from filename\n",
        "def get_file_name_pipeline_history_min_and_max(project_id,file_path):\n",
        "\n",
        "  client = storage.Client(project_id)\n",
        "\n",
        "  bucket_name = file_path.split('/')[2]\n",
        "  prefix      = file_path.split(bucket)[1][1:-1]\n",
        "\n",
        "  print(bucket_name)\n",
        "  print(prefix)\n",
        "\n",
        "  #create a list of date where we pick the oldest and the newest\n",
        "  blobs = [[blob.name.split('/')[-1].split('__')[-1]] for blob in client.list_blobs(\n",
        "  bucket_name,\n",
        "  prefix = prefix)]\n",
        "\n",
        "  #extract the max and min date\n",
        "\n",
        "  newest = sorted(blobs, key=lambda tup: tup[0])[-1] \n",
        "  oldest = sorted(blobs, key=lambda tup: tup[0])[0]\n",
        "\n",
        "  return oldest,newest,blobs\n",
        "\n",
        "def check_missing_days(list1,list2):\n",
        "  #function that checks if all days are present \n",
        "  #list1 contains the set that needs to be checked\n",
        "  #list2 contains the whole list\n",
        "\n",
        "  missings = []\n",
        "\n",
        "  for item in list2:\n",
        "\n",
        "    if item in list1:\n",
        "      pass\n",
        "    else:\n",
        "      missings.append(item)\n",
        "\n",
        "  return missings"
      ],
      "metadata": {
        "id": "PIAxVwr66JUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "project_id = 'advanced-analytics-278408'\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = '''\n",
        "SELECT\n",
        "*\n",
        "FROM `advanced-analytics-278408.data__1st_layer.9891__monitoring_information_and_params`\n",
        "WHERE is_to_be_monitored = true\n",
        "and landing_zone = 'GCS';\n",
        "'''\n",
        "\n",
        "df = client.query(query).to_dataframe()\n",
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQw9AbRLZ-Gh",
        "outputId": "57ca9d55-5018-42e5-b9fc-a83dd226f1dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['project_id', 'dataset_id', 'table_id', 'complete_table_id',\n",
              "       'table_code', 'is_to_be_monitored', 'current_development_status',\n",
              "       'current_development_status_id', 'source_system_id',\n",
              "       'source_system_name', 'source_system_ownership_type', 'extraction_tool',\n",
              "       'extraction_script', 'extraction_is_scheduled',\n",
              "       'extraction_schedule_pattern', 'landing_zone', 'landing_zone_path',\n",
              "       'landing_zone_form', 'loading_tool', 'load_script', 'load_is_scheduled',\n",
              "       'load_schedule_pattern', 'load_form', 'is_dx_framework_applied',\n",
              "       'id_field', 'data_loop_field', 'data_loop_granularity',\n",
              "       'overall_documentation_URL', 'to_do_pending', 'notes'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[0,'My_new_col'] = 'new_inserted_value'"
      ],
      "metadata": {
        "id": "PdPEHGOxbCOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_2 = '''\n",
        "SELECT\n",
        "*\n",
        "FROM `advanced-analytics-278408.user__rubini_riccardo.9898__pipelines_target_landing_zone_metadata`;\n",
        "'''\n",
        "\n",
        "df_9898 = client.query(query_2).to_dataframe()"
      ],
      "metadata": {
        "id": "PSv9XGMjcGU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_9898.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-nkujuNcYfk",
        "outputId": "3ef31cf4-d41b-4304-e500-4a6b9891fbbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "analysis_date                                      object\n",
              "analysis_date_time                                 object\n",
              "project_id                                         object\n",
              "dataset_id                                         object\n",
              "table_id                                           object\n",
              "complete_table_id                                  object\n",
              "table_code                                         object\n",
              "is_to_be_monitored                                 object\n",
              "current_development_status                         object\n",
              "current_development_status_id                      object\n",
              "source_system_id                                   object\n",
              "source_system_name                                 object\n",
              "source_system_ownership_type                       object\n",
              "extraction_tool                                    object\n",
              "extraction_script                                  object\n",
              "extraction_is_scheduled                            object\n",
              "extraction_schedule_pattern                        object\n",
              "landing_zone                                       object\n",
              "landing_zone_path                                  object\n",
              "landing_zone_form                                  object\n",
              "loading_tool                                       object\n",
              "load_script                                        object\n",
              "load_is_scheduled                                  object\n",
              "load_schedule_pattern                              object\n",
              "load_form                                          object\n",
              "bucket_id                                          object\n",
              "bucket_name                                        object\n",
              "bucket_created                                     object\n",
              "bucket_location                                    object\n",
              "bucket_location_type                               object\n",
              "bucket_storage_class                               object\n",
              "file_path_policy_respected                         object\n",
              "file_path_source_system                            object\n",
              "file_path_pipeline_code                            object\n",
              "file_path_pipeline_code_distinct_versions          object\n",
              "file_path_pipeline_code_distinct_versions_sizes    object\n",
              "file_path_pipeline_code_checked_version            object\n",
              "file_number_in_landing_zone_path                   object\n",
              "file_size_in_landing_zone_path                     object\n",
              "file_oldest_creation_datetime                      object\n",
              "file_newest_creation_datetime                      object\n",
              "file_name_policy_respected                         object\n",
              "file_name_pipeline_code_info                       object\n",
              "file_name_pipeline_name_info                       object\n",
              "file_name_pipeline_windowframe_info                object\n",
              "file_name_pipeline_history_min_info                object\n",
              "file_name_pipeline_history_max_info                object\n",
              "files_period_covered                               object\n",
              "files_period_missing_between_min_max               object\n",
              "warning_flag_status                                object\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "#fill the first N column of df_9898 with the first N columns of 9891__monitoring_information_and_params\n",
        "#define a list of columns to fill \n",
        "\n",
        "columns_list = [\n",
        "      'project_id', 'dataset_id', 'table_id', 'complete_table_id',\n",
        "       'table_code', 'is_to_be_monitored', 'current_development_status',\n",
        "       'current_development_status_id', 'source_system_id',\n",
        "       'source_system_name', 'source_system_ownership_type', 'extraction_tool',\n",
        "       'extraction_script', 'extraction_is_scheduled',\n",
        "       'extraction_schedule_pattern', 'landing_zone', 'landing_zone_path',\n",
        "       'landing_zone_form', 'loading_tool', 'load_script', 'load_is_scheduled',\n",
        "       'load_schedule_pattern', 'load_form'\n",
        "]\n",
        "\n",
        "\n",
        "#loop over the columns to fill \n",
        "\n",
        "for i in range(10):\n",
        "\n",
        "  #print the filepath analised\n",
        "  print(\"Computing row : \" + str(i) + \" of \" + str(df.shape[0]))\n",
        "  print(df.loc[i,'landing_zone_path'])\n",
        "\n",
        "  #fill the date\n",
        "\n",
        "  df_9898.loc[i,'analysis_date'] = datetime.datetime.now().strftime('%Y-%m-%d')\n",
        "  df_9898.loc[i,'analysis_date_time'] = datetime.datetime.now()\n",
        "\n",
        "  #add columns from the 9891\n",
        "  for col in columns_list:\n",
        "\n",
        "    df_9898.loc[i,col] = df.loc[i,col]\n",
        "\n",
        "  bucket = df.loc[i,'landing_zone_path'].split('/')[2]\n",
        "  bucket_meta = bucket_metadata(bucket,project_id)\n",
        "  \n",
        "  #add bucket metadata\n",
        "  for key in bucket_meta:\n",
        "    df_9898.loc[i,key] = bucket_meta[key] \n",
        "\n",
        "  #check that the policy and path info\n",
        "  df_9898.loc[i,'file_path_policy_respected'] = file_path_policy_respected_check(df_9898.loc[i,'landing_zone_path'])\n",
        "  df_9898.loc[i,'file_path_source_system'] = df_9898.loc[i,'landing_zone_path'].split('/')[5]\n",
        "  df_9898.loc[i,'file_path_pipeline_code'] = df_9898.loc[i,'landing_zone_path'].split('/')[4]\n",
        "\n",
        "  #extract bucket and prefix for a given landing_zone_path\n",
        "  bucket = df_9898.loc[i,'landing_zone_path'].split('/')[2]\n",
        "  prefix  = df_9898.loc[i,'landing_zone_path'].split(bucket)[1][1:-4]\n",
        "  #count the number of v1 v2 v3 \n",
        "  print(bucket)\n",
        "  print(prefix)\n",
        "  print(list_directories(bucket,prefix))\n",
        "\n",
        "  df_9898.loc[i,'file_path_pipeline_code_distinct_versions'] = len(list_directories(bucket,prefix))\n",
        "\n",
        "  #compute the size of each versioning of each folder\n",
        "\n",
        "  df_9898.loc[i,'file_path_pipeline_code_distinct_versions_sizes'] = str(file_path_pipeline_code_distinct_versions_check(df_9898.loc[i,'landing_zone_path']))\n",
        "  df_9898.loc[i,'file_path_pipeline_code_checked_version'] = 'TO BE INSERTED'\n",
        "\n",
        "  #compute the number of files in the landing zone path\n",
        "\n",
        "  df_9898.loc[i,'file_number_in_landing_zone_path'] = get_file_number_in_landing_zone_path(df_9898.loc[i,'landing_zone_path'])\n",
        " \n",
        "  df_9898.loc[i,'file_size_in_landing_zone_path'] = 'TO BE INSERTED'\n",
        "\n",
        "  #get oldest and most recent file in folder\n",
        "\n",
        "  new = get_oldest_and_newest_file(df_9898.loc[i,'landing_zone_path'],project_id)[0][1]\n",
        "  old = get_oldest_and_newest_file(df_9898.loc[i,'landing_zone_path'],project_id)[1][1]\n",
        "\n",
        "  df_9898.loc[i,'file_oldest_creation_datetime'] = old\n",
        "  df_9898.loc[i,'file_newest_creation_datetime'] = new\n",
        "\n",
        "  #check policy of the file\n",
        "  df_9898.loc[i,'file_name_policy_respected']  = is_file_policy_respected(df_9898.loc[i,'landing_zone_path'],project_id)\n",
        "\n",
        "  #extract file infos\n",
        "\n",
        "  files_info = extract_file_info(df_9898.loc[i,'landing_zone_path'],project_id)\n",
        "\n",
        "  df_9898.loc[i,'file_name_pipeline_code_info']        =  files_info['file_name_pipeline_code_info']\n",
        "  df_9898.loc[i,'file_name_pipeline_name_info']        =  files_info['file_name_pipeline_name_info']\n",
        "  df_9898.loc[i,'file_name_pipeline_windowframe_info'] =  files_info['file_name_pipeline_windowframe_info']\n",
        "\n",
        "  #get first and latest date from filename\n",
        "\n",
        "  old,new,list_day = get_file_name_pipeline_history_min_and_max(project_id,df_9898.loc[i,'landing_zone_path'])\n",
        "  period_covered = [old[0],new[0]]\n",
        "\n",
        "  df_9898.loc[i,'file_name_pipeline_history_min_info'] = old[0]\n",
        "  df_9898.loc[i,'file_name_pipeline_history_max_info'] = new[0]\n",
        "  df_9898.loc[i,'files_period_covered'] = str(period_covered)\n",
        "\n",
        "  #get the complete list of days\n",
        "  try : \n",
        "    complete_list_days = generate_list(period_covered,df_9898.loc[i,'file_name_pipeline_windowframe_info'])\n",
        "    missings = check_missing_days(np.asarray(list_day)[:,0],complete_list_days)\n",
        "\n",
        "    if len(missings) == 0 : df_9898.loc[i,'files_period_missing_between_min_max'] = 'No missing days'\n",
        "    else: df_9898.loc[i,'files_period_missing_between_min_max'] = str(missings)\n",
        "  except:\n",
        "    print(\"Missing Files, step skipped\")\n",
        "  \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQJmdc9YckAG",
        "outputId": "96c6040b-6720-4b0f-e04a-d6274199af58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing row : 0 of 48\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0004/v3/\n",
            "['swarovski-advanced-analytics-data-ingestion-test', 'clean', '00_SAP_P30', '0004', 'v3']\n",
            "swarovski-advanced-analytics-data-ingestion-test\n",
            "clean/00_SAP_P30/0004\n",
            "['clean/00_SAP_P30/0004/v1/', 'clean/00_SAP_P30/0004/v2/', 'clean/00_SAP_P30/0004/v3/']\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0004/v1/\n",
            "Size : ['552271279', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0004/v1']\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0004/v2/\n",
            "Size : ['84698956', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0004/v2']\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0004/v3/\n",
            "Size : ['233904036', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0004/v3']\n",
            "Landing zone path : gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0004/v3/\n",
            "File number : 3987\n",
            "Check that the policy of the file is respected\n",
            "Bucket : swarovski-advanced-analytics-data-ingestion-test\n",
            "Prefix : clean/00_SAP_P30/0004\n",
            "swarovski-advanced-analytics-data-ingestion-test\n",
            "clean/00_SAP_P30/0004/v3\n",
            "Missing Files, step skipped\n",
            "Computing row : 1 of 48\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0006/v2/\n",
            "['swarovski-advanced-analytics-data-ingestion-test', 'clean', '00_SAP_P30', '0006', 'v2']\n",
            "swarovski-advanced-analytics-data-ingestion-test\n",
            "clean/00_SAP_P30/0006\n",
            "['clean/00_SAP_P30/0006/v0/', 'clean/00_SAP_P30/0006/v1/', 'clean/00_SAP_P30/0006/v2/']\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0006/v0/\n",
            "Size : ['160705', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0006/v0']\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0006/v1/\n",
            "Size : ['20401428471', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0006/v1']\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0006/v2/\n",
            "Size : ['16806465304', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0006/v2']\n",
            "Landing zone path : gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0006/v2/\n",
            "File number : 1793\n",
            "Check that the policy of the file is respected\n",
            "Bucket : swarovski-advanced-analytics-data-ingestion-test\n",
            "Prefix : clean/00_SAP_P30/0006\n",
            "swarovski-advanced-analytics-data-ingestion-test\n",
            "clean/00_SAP_P30/0006/v2\n",
            "Missing Files, step skipped\n",
            "Computing row : 2 of 48\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0019/v1/\n",
            "['swarovski-advanced-analytics-data-ingestion-test', 'clean', '00_SAP_P30', '0019', 'v1']\n",
            "swarovski-advanced-analytics-data-ingestion-test\n",
            "clean/00_SAP_P30/0019\n",
            "['clean/00_SAP_P30/0019/v1/']\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0019/v1/\n",
            "Size : ['27959954497', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0019/v1']\n",
            "Landing zone path : gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0019/v1/\n",
            "File number : 1793\n",
            "Check that the policy of the file is respected\n",
            "Bucket : swarovski-advanced-analytics-data-ingestion-test\n",
            "Prefix : clean/00_SAP_P30/0019\n",
            "swarovski-advanced-analytics-data-ingestion-test\n",
            "clean/00_SAP_P30/0019/v1\n",
            "Missing Files, step skipped\n",
            "Computing row : 3 of 48\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0003/v5/\n",
            "['swarovski-advanced-analytics-data-ingestion-test', 'clean', '00_SAP_P30', '0003', 'v5']\n",
            "swarovski-advanced-analytics-data-ingestion-test\n",
            "clean/00_SAP_P30/0003\n",
            "['clean/00_SAP_P30/0003/v0/', 'clean/00_SAP_P30/0003/v1/', 'clean/00_SAP_P30/0003/v2/', 'clean/00_SAP_P30/0003/v3/', 'clean/00_SAP_P30/0003/v4/', 'clean/00_SAP_P30/0003/v5/']\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0003/v0/\n",
            "Size : ['546424', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0003/v0']\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0003/v1/\n",
            "Size : ['364990009', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0003/v1']\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0003/v2/\n",
            "Size : ['519847320', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0003/v2']\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0003/v3/\n",
            "Size : ['580060761', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0003/v3']\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0003/v4/\n",
            "Size : ['820492758', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0003/v4']\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0003/v5/\n",
            "Size : ['1095807827', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0003/v5']\n",
            "Landing zone path : gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0003/v5/\n",
            "File number : 1794\n",
            "Check that the policy of the file is respected\n",
            "Bucket : swarovski-advanced-analytics-data-ingestion-test\n",
            "Prefix : clean/00_SAP_P30/0003\n",
            "swarovski-advanced-analytics-data-ingestion-test\n",
            "clean/00_SAP_P30/0003/v5\n",
            "Missing Files, step skipped\n",
            "Computing row : 4 of 48\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0014/v1/\n",
            "['swarovski-advanced-analytics-data-ingestion-test', 'clean', '00_SAP_P30', '0014', 'v1']\n",
            "swarovski-advanced-analytics-data-ingestion-test\n",
            "clean/00_SAP_P30/0014\n",
            "['clean/00_SAP_P30/0014/v1/']\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0014/v1/\n",
            "Size : ['642529370', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0014/v1']\n",
            "Landing zone path : gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0014/v1/\n",
            "File number : 131\n",
            "Check that the policy of the file is respected\n",
            "Bucket : swarovski-advanced-analytics-data-ingestion-test\n",
            "Prefix : clean/00_SAP_P30/0014\n",
            "swarovski-advanced-analytics-data-ingestion-test\n",
            "clean/00_SAP_P30/0014/v1\n",
            "Missing Files, step skipped\n",
            "Computing row : 5 of 48\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0015/v1/\n",
            "['swarovski-advanced-analytics-data-ingestion-test', 'clean', '00_SAP_P30', '0015', 'v1']\n",
            "swarovski-advanced-analytics-data-ingestion-test\n",
            "clean/00_SAP_P30/0015\n",
            "['clean/00_SAP_P30/0015/v1/']\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0015/v1/\n",
            "Size : ['44034634677', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0015/v1']\n",
            "Landing zone path : gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0015/v1/\n",
            "File number : 3984\n",
            "Check that the policy of the file is respected\n",
            "Bucket : swarovski-advanced-analytics-data-ingestion-test\n",
            "Prefix : clean/00_SAP_P30/0015\n",
            "swarovski-advanced-analytics-data-ingestion-test\n",
            "clean/00_SAP_P30/0015/v1\n",
            "Missing Files, step skipped\n",
            "Computing row : 6 of 48\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0005/v1/\n",
            "['swarovski-advanced-analytics-data-ingestion-test', 'clean', '00_SAP_P30', '0005', 'v1']\n",
            "swarovski-advanced-analytics-data-ingestion-test\n",
            "clean/00_SAP_P30/0005\n",
            "['clean/00_SAP_P30/0005/v0/', 'clean/00_SAP_P30/0005/v1/', 'clean/00_SAP_P30/0005/v2/', 'clean/00_SAP_P30/0005/v3/', 'clean/00_SAP_P30/0005/v4/', 'clean/00_SAP_P30/0005/v5/', 'clean/00_SAP_P30/0005/v6/', 'clean/00_SAP_P30/0005/v7/']\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0005/v0/\n",
            "Size : ['9035263', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0005/v0']\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0005/v1/\n",
            "Size : ['1209971358', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0005/v1']\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0005/v2/\n",
            "Size : ['1209617015', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0005/v2']\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0005/v3/\n",
            "Size : ['4154311853', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0005/v3']\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0005/v4/\n",
            "Size : ['9504385543', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0005/v4']\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0005/v5/\n",
            "Size : ['57145976884', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0005/v5']\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0005/v6/\n",
            "Size : ['10089830039', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0005/v6']\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0005/v7/\n",
            "Size : ['15318628283', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0005/v7']\n",
            "Landing zone path : gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0005/v1/\n",
            "File number : 1063\n",
            "Check that the policy of the file is respected\n",
            "Bucket : swarovski-advanced-analytics-data-ingestion-test\n",
            "Prefix : clean/00_SAP_P30/0005\n",
            "Found not conformal file\n",
            "clean/00_SAP_P30/0005/v5/2021/0005__online_sales__dd__2021-02-18__20210208\n",
            "swarovski-advanced-analytics-data-ingestion-test\n",
            "clean/00_SAP_P30/0005/v1\n",
            "Missing Files, step skipped\n",
            "Computing row : 7 of 48\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0016/v1/\n",
            "['swarovski-advanced-analytics-data-ingestion-test', 'clean', '00_SAP_P30', '0016', 'v1']\n",
            "swarovski-advanced-analytics-data-ingestion-test\n",
            "clean/00_SAP_P30/0016\n",
            "['clean/00_SAP_P30/0016/v1/']\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0016/v1/\n",
            "Size : ['2979496', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0016/v1']\n",
            "Landing zone path : gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0016/v1/\n",
            "File number : 60\n",
            "Check that the policy of the file is respected\n",
            "Bucket : swarovski-advanced-analytics-data-ingestion-test\n",
            "Prefix : clean/00_SAP_P30/0016\n",
            "swarovski-advanced-analytics-data-ingestion-test\n",
            "clean/00_SAP_P30/0016/v1\n",
            "Missing Files, step skipped\n",
            "Computing row : 8 of 48\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0017/v1/\n",
            "['swarovski-advanced-analytics-data-ingestion-test', 'clean', '00_SAP_P30', '0017', 'v1']\n",
            "swarovski-advanced-analytics-data-ingestion-test\n",
            "clean/00_SAP_P30/0017\n",
            "['clean/00_SAP_P30/0017/v0/', 'clean/00_SAP_P30/0017/v1/', 'clean/00_SAP_P30/0017/v2/', 'clean/00_SAP_P30/0017/v3/']\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0017/v0/\n",
            "Size : ['602014971', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0017/v0']\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0017/v1/\n",
            "Size : ['1574571317', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0017/v1']\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0017/v2/\n",
            "Size : ['1263764816', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0017/v2']\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0017/v3/\n",
            "Size : ['1076928209', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0017/v3']\n",
            "Landing zone path : gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0017/v1/\n",
            "File number : 50\n",
            "Check that the policy of the file is respected\n",
            "Bucket : swarovski-advanced-analytics-data-ingestion-test\n",
            "Prefix : clean/00_SAP_P30/0017\n",
            "Found not conformal file\n",
            "clean/00_SAP_P30/0017/v0/20222022/0017__profit_and_loss_v2__mm__testing__loop202210\n",
            "swarovski-advanced-analytics-data-ingestion-test\n",
            "clean/00_SAP_P30/0017/v1\n",
            "Missing Files, step skipped\n",
            "Computing row : 9 of 48\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0018/v1/\n",
            "['swarovski-advanced-analytics-data-ingestion-test', 'clean', '00_SAP_P30', '0018', 'v1']\n",
            "swarovski-advanced-analytics-data-ingestion-test\n",
            "clean/00_SAP_P30/0018\n",
            "['clean/00_SAP_P30/0018/v1/']\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0018/v1/\n",
            "Size : ['49640927', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0018/v1']\n",
            "Landing zone path : gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0018/v1/\n",
            "File number : 59\n",
            "Check that the policy of the file is respected\n",
            "Bucket : swarovski-advanced-analytics-data-ingestion-test\n",
            "Prefix : clean/00_SAP_P30/0018\n",
            "swarovski-advanced-analytics-data-ingestion-test\n",
            "clean/00_SAP_P30/0018/v1\n",
            "Missing Files, step skipped\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#select the oldest and newest file -> from filename\n",
        "def get_file_name_pipeline_history_min_and_max(project_id,file_path):\n",
        "\n",
        "  client = storage.Client(project_id)\n",
        "\n",
        "  bucket_name = file_path.split('/')[2]\n",
        "  prefix      = file_path.split(bucket)[1][1:-1]\n",
        "\n",
        "  print(bucket_name)\n",
        "  print(prefix)\n",
        "\n",
        "  #create a list of date where we pick the oldest and the newest\n",
        "  blobs = [[blob.name.split('/')[-1].split('__')[-1]] for blob in client.list_blobs(\n",
        "  bucket_name,\n",
        "  prefix = prefix)]\n",
        "\n",
        "  #extract the max and min date\n",
        "\n",
        "  newest = sorted(blobs, key=lambda tup: tup[0])[-1] \n",
        "  oldest = sorted(blobs, key=lambda tup: tup[0])[0]\n",
        "\n",
        "  return oldest,newest,blobs\n",
        "\n",
        "#get_file_name_pipeline_history_min_and_max(project_id,df_9898.loc[0,'landing_zone_path']) #test daily load\n",
        "\n",
        "old,new,list_day = get_file_name_pipeline_history_min_and_max(project_id,df_9898.loc[4,'landing_zone_path'])  #test monthly load\n",
        "\n",
        "#compute file period covered \n",
        "\n",
        "period_covered = [old[0],new[0]]\n",
        "period_covered\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwOvRtNJfZVW",
        "outputId": "871be2f5-c96d-46c3-a071-0700b700be1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "swarovski-advanced-analytics-data-ingestion-test\n",
            "clean/00_SAP_P30/0014/v1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['201201', '202211']"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#generate list of days from the period covered\n",
        "\n",
        "from datetime import datetime, date, timedelta\n",
        "\n",
        "def generate_list(period_covered,load_time_frame):\n",
        "\n",
        "  year_s = int(period_covered[0][0:4])\n",
        "  year_e = int(period_covered[1][0:4])\n",
        "\n",
        "  month_s = int(period_covered[0][4:6])\n",
        "  month_e = int(period_covered[1][4:6])\n",
        "\n",
        "  if load_time_frame == 'dd':\n",
        "\n",
        "    day_s = int(period_covered[0][6:8])\n",
        "    day_e = int(period_covered[1][6:8])\n",
        "    freq = 'd'\n",
        "\n",
        "  elif load_time_frame == 'mm':\n",
        "\n",
        "    day_s =  1\n",
        "    day_e =  1\n",
        "    freq = 'm'\n",
        "\n",
        "  start = date(year_s, month_s, day_s)  \n",
        "  end   = date(year_e, month_e, day_e) \n",
        "  \n",
        "  time_list = []\n",
        "\n",
        "  if load_time_frame == 'mm':\n",
        "    \n",
        "    for item in pd.date_range(start,end,freq=freq).strftime('%Y%m%d').to_list():\n",
        "\n",
        "      time_list.append(item[0:6])\n",
        "  \n",
        "  elif load_time_frame == 'dd':\n",
        "\n",
        "    time_list = pd.date_range(start,end,freq=freq).strftime('%Y%m%d').to_list() \n",
        "\n",
        "  \n",
        "  return time_list\n",
        "    \n",
        "\n",
        "complete_list = generate_list(period_covered,'mm')\n",
        "complete_list"
      ],
      "metadata": {
        "id": "hfcYazsyzQuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check lists \n",
        "def check_missing_days(list1,list2):\n",
        "  #function that checks if all days are present \n",
        "  #list1 contains the set that needs to be checked\n",
        "  #list2 contains the whole list\n",
        "\n",
        "  missings = []\n",
        "\n",
        "  for item in list2:\n",
        "\n",
        "    if item in list1:\n",
        "      pass\n",
        "    else:\n",
        "      missings.append(item)\n",
        "\n",
        "  return missings\n",
        "\n"
      ],
      "metadata": {
        "id": "Q2fGZyEjAPdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "for i in range(len(complete_list)):\n",
        "  print(complete_list[i],list_day[i][0])"
      ],
      "metadata": {
        "id": "u0UjN7tXCD78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missings = check_missing_days(np.asarray(list_day)[:,0],complete_list)\n",
        "\n",
        "if len(missings) == 0 : is_list_empty = True\n",
        "else: is_list_empty = False"
      ],
      "metadata": {
        "id": "PI92BSIdAoXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missings\n",
        "is_list_empty"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ur8Z51nlBKtZ",
        "outputId": "276c3f81-b8e7-4684-9aa9-10c4bd543300"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_id = 'user__rubini_riccardo'\n",
        "\n",
        "dataset_ref = client.dataset(dataset_id)\n",
        "job_config = bigquery.LoadJobConfig()\n",
        "job_config.autodetect = False\n",
        "job_config.write_disposition = \"WRITE_TRUNCATE\"\n",
        "#job_config.source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n",
        "load_job = client.load_table_from_dataframe(df_9898, dataset_ref.table(\"9898__pipelines_target_landing_zone_metadata_v1\"), job_config=job_config, location=\"EU\")    # API request\n",
        "print(\"Starting job {}\".format(load_job))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-W-E_wSlaqHG",
        "outputId": "ab045dc3-e983-4f3c-95fd-97a89ef0cccb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/google/cloud/bigquery/_pandas_helpers.py:552: UserWarning: Pyarrow could not determine the type of columns: bucket_created, files_period_missing_between_min_max, warning_flag_status.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting job LoadJob<project=advanced-analytics-278408, location=EU, id=2e1f0dc1-23ce-4de8-8b18-b5cf367c0946>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IV_F9p_GfrKy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}