{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "76kDLASjEmSv"
      },
      "outputs": [],
      "source": [
        "from google.api_core import page_iterator\n",
        "from google.cloud import storage\n",
        "import os\n",
        "import pandas as pd \n",
        "from google.cloud import bigquery"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "print('Authenticated')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsSPjTu_Z8R7",
        "outputId": "5c24a75a-70b2-44e9-c8d0-c95723c301df"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authenticated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#next step is to add the extra columns\n",
        "\n",
        "def bucket_metadata(bucket_name,project_id):\n",
        "    \"\"\"Prints out a bucket's metadata.\"\"\"\n",
        "  \n",
        "    storage_client = storage.Client(project_id)\n",
        "    bucket = storage_client.get_bucket(bucket_name)\n",
        "\n",
        "    #print(f\"ID: {bucket.id}\")\n",
        "    #print(f\"Name: {bucket.name}\")\n",
        "    #print(f\"Storage Class: {bucket.storage_class}\")\n",
        "    #print(f\"Location: {bucket.location}\")\n",
        "    #print(f\"Location Type: {bucket.location_type}\")\n",
        "\n",
        "    data                  = {}\n",
        "    data['bucket_id']     = bucket.id\n",
        "    data['bucket_name']          = bucket.name\n",
        "    data['bucket_storage_class'] = bucket.storage_class\n",
        "    data['bucket_location']      = bucket.location\n",
        "    data['bucket_location_type'] = bucket.location_type\n",
        "    #json_data = json.dumps(data)\n",
        "\n",
        "    return data\n",
        "\n",
        "def file_path_policy_respected_check(file_path):\n",
        "    #function that checks whether the path policy of a certain bucket is respected or not\n",
        "    #each path should be in the following format \n",
        "    path_chuncks = file_path.split('/')[2:-1]\n",
        "\n",
        "    if len(path_chuncks) == 5 : policy_respected = True\n",
        "    else : policy_respected = False\n",
        "\n",
        "    return policy_respected\n",
        "\n",
        "def _item_to_value(iterator, item):\n",
        "    return item\n",
        "\n",
        "def list_directories(bucket_name, prefix):\n",
        "\n",
        "    if prefix and not prefix.endswith('/'):\n",
        "        prefix += '/'\n",
        "\n",
        "    extra_params = {\n",
        "        \"projection\": \"noAcl\",\n",
        "        \"prefix\": prefix,\n",
        "        \"delimiter\": '/'\n",
        "    }\n",
        "\n",
        "    gcs = storage.Client()\n",
        "\n",
        "    path = \"/b/\" + bucket_name + \"/o\"\n",
        "\n",
        "    iterator = page_iterator.HTTPIterator(\n",
        "        client=gcs,\n",
        "        api_request=gcs._connection.api_request,\n",
        "        path=path,\n",
        "        items_key='prefixes',\n",
        "        item_to_value=_item_to_value,\n",
        "        extra_params=extra_params,\n",
        "    )\n",
        "\n",
        "    return [x for x in iterator]"
      ],
      "metadata": {
        "id": "WuemepCphvDc"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#definitions of helpers functions\n",
        "\n",
        "def file_path_pipeline_code_distinct_versions_check(input_path):\n",
        "\n",
        "  bucket = input_path.split('/')[2]\n",
        "  prefix  = input_path.split(bucket)[1][1:-4]\n",
        "\n",
        "  #build a list containing path and corresponding sizes\n",
        "\n",
        "  versions_sizes = list()\n",
        "\n",
        "  for directory in list_directories(bucket,prefix):\n",
        "\n",
        "    path = \"gs://\"+ bucket + \"/\" +  directory \n",
        "    print(path)\n",
        "    size = os.popen(\"gsutil du -s \" + str(path)).read().split()\n",
        "    print(\"Size : \" + str(size))\n",
        "\n",
        "    versions_sizes.append(size)\n",
        "  return versions_sizes\n"
      ],
      "metadata": {
        "id": "XwD7H4xvQxVd"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "project_id = 'advanced-analytics-278408'\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = '''\n",
        "SELECT\n",
        "*\n",
        "FROM `advanced-analytics-278408.data__1st_layer.9891__monitoring_information_and_params`\n",
        "WHERE is_to_be_monitored = true\n",
        "and landing_zone = 'GCS';\n",
        "'''\n",
        "\n",
        "df = client.query(query).to_dataframe()\n",
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQw9AbRLZ-Gh",
        "outputId": "19632bf8-46a7-4985-e599-e938811c1cd2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['project_id', 'dataset_id', 'table_id', 'complete_table_id',\n",
              "       'table_code', 'is_to_be_monitored', 'current_development_status',\n",
              "       'current_development_status_id', 'source_system_id',\n",
              "       'source_system_name', 'source_system_ownership_type', 'extraction_tool',\n",
              "       'extraction_script', 'extraction_is_scheduled',\n",
              "       'extraction_schedule_pattern', 'landing_zone', 'landing_zone_path',\n",
              "       'landing_zone_form', 'loading_tool', 'load_script', 'load_is_scheduled',\n",
              "       'load_schedule_pattern', 'load_form', 'is_dx_framework_applied',\n",
              "       'id_field', 'data_loop_field', 'data_loop_granularity',\n",
              "       'overall_documentation_URL', 'to_do_pending', 'notes'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[0,'My_new_col'] = 'new_inserted_value'"
      ],
      "metadata": {
        "id": "PdPEHGOxbCOz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_2 = '''\n",
        "SELECT\n",
        "*\n",
        "FROM `advanced-analytics-278408.user__rubini_riccardo.9898__pipelines_target_landing_zone_metadata`;\n",
        "'''\n",
        "\n",
        "df_9898 = client.query(query_2).to_dataframe()"
      ],
      "metadata": {
        "id": "PSv9XGMjcGU8"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_9898.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-nkujuNcYfk",
        "outputId": "2156173a-58eb-42a2-a962-c5de302cd9f3"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "analysis_date                                      object\n",
              "analysis_date_time                                 object\n",
              "project_id                                         object\n",
              "dataset_id                                         object\n",
              "table_id                                           object\n",
              "complete_table_id                                  object\n",
              "table_code                                         object\n",
              "is_to_be_monitored                                 object\n",
              "current_development_status                         object\n",
              "current_development_status_id                      object\n",
              "source_system_id                                   object\n",
              "source_system_name                                 object\n",
              "source_system_ownership_type                       object\n",
              "extraction_tool                                    object\n",
              "extraction_script                                  object\n",
              "extraction_is_scheduled                            object\n",
              "extraction_schedule_pattern                        object\n",
              "landing_zone                                       object\n",
              "landing_zone_path                                  object\n",
              "landing_zone_form                                  object\n",
              "loading_tool                                       object\n",
              "load_script                                        object\n",
              "load_is_scheduled                                  object\n",
              "load_schedule_pattern                              object\n",
              "load_form                                          object\n",
              "bucket_id                                          object\n",
              "bucket_name                                        object\n",
              "bucket_created                                     object\n",
              "bucket_location                                    object\n",
              "bucket_location_type                               object\n",
              "bucket_storage_class                               object\n",
              "file_path_policy_respected                         object\n",
              "file_path_source_system                            object\n",
              "file_path_pipeline_code                            object\n",
              "file_path_pipeline_code_distinct_versions          object\n",
              "file_path_pipeline_code_distinct_versions_sizes    object\n",
              "file_path_pipeline_code_checked_version            object\n",
              "file_number_in_landing_zone_path                   object\n",
              "file_size_in_landing_zone_path                     object\n",
              "file_oldest_creation_datetime                      object\n",
              "file_newest_creation_datetime                      object\n",
              "file_name_policy_respected                         object\n",
              "file_name_pipeline_code_info                       object\n",
              "file_name_pipeline_name_info                       object\n",
              "file_name_pipeline_windowframe_info                object\n",
              "file_name_pipeline_history_min_info                object\n",
              "file_name_pipeline_history_max_info                object\n",
              "files_period_covered                               object\n",
              "files_period_missing_between_min_max               object\n",
              "warning_flag_status                                object\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "#fill the first N column of df_9898 with the first N columns of 9891__monitoring_information_and_params\n",
        "#define a list of columns to fill \n",
        "\n",
        "columns_list = [\n",
        "      'project_id', 'dataset_id', 'table_id', 'complete_table_id',\n",
        "       'table_code', 'is_to_be_monitored', 'current_development_status',\n",
        "       'current_development_status_id', 'source_system_id',\n",
        "       'source_system_name', 'source_system_ownership_type', 'extraction_tool',\n",
        "       'extraction_script', 'extraction_is_scheduled',\n",
        "       'extraction_schedule_pattern', 'landing_zone', 'landing_zone_path',\n",
        "       'landing_zone_form', 'loading_tool', 'load_script', 'load_is_scheduled',\n",
        "       'load_schedule_pattern', 'load_form'\n",
        "]\n",
        "\n",
        "\n",
        "#loop over the columns to fill \n",
        "\n",
        "for i in range(2):\n",
        "\n",
        "  #print the filepath analised\n",
        "  print(df.loc[i,'landing_zone_path'])\n",
        "\n",
        "  #fill the date\n",
        "\n",
        "  df_9898.loc[i,'analysis_date'] = datetime.datetime.now().strftime('%Y-%m-%d')\n",
        "  df_9898.loc[i,'analysis_date_time'] = datetime.datetime.now()\n",
        "\n",
        "  #add columns from the 9891\n",
        "  for col in columns_list:\n",
        "\n",
        "    df_9898.loc[i,col] = df.loc[i,col]\n",
        "\n",
        "  bucket = df.loc[i,'landing_zone_path'].split('/')[2]\n",
        "  bucket_meta = bucket_metadata(bucket,project_id)\n",
        "  \n",
        "  #add bucket metadata\n",
        "  for key in bucket_meta:\n",
        "    df_9898.loc[i,key] = bucket_meta[key] \n",
        "\n",
        "  #check that the policy and path info\n",
        "  df_9898.loc[i,'file_path_policy_respected'] = file_path_policy_respected_check(df_9898.loc[i,'landing_zone_path'])\n",
        "  df_9898.loc[i,'file_path_source_system'] = df_9898.loc[i,'landing_zone_path'].split('/')[5]\n",
        "  df_9898.loc[i,'file_path_pipeline_code'] = df_9898.loc[i,'landing_zone_path'].split('/')[4]\n",
        "\n",
        "  #extract bucket and prefix for a given landing_zone_path\n",
        "  bucket = df_9898.loc[i,'landing_zone_path'].split('/')[2]\n",
        "  prefix  = df_9898.loc[i,'landing_zone_path'].split(bucket)[1][1:-4]\n",
        "  #count the number of v1 v2 v3 \n",
        "  print(bucket)\n",
        "  print(prefix)\n",
        "  print(list_directories(bucket,prefix))\n",
        "\n",
        "  df_9898.loc[i,'file_path_pipeline_code_distinct_versions'] = len(list_directories(bucket,prefix))\n",
        "\n",
        "  #compute the size of each versioning of each folder\n",
        "\n",
        "  df_9898.loc[i,'file_path_pipeline_code_distinct_versions_sizes'] = str(file_path_pipeline_code_distinct_versions_check(df_9898.loc[i,'landing_zone_path']))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQJmdc9YckAG",
        "outputId": "92d0053d-3cd3-4c9b-df24-bd5588f8f49d"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0004/v3/\n",
            "swarovski-advanced-analytics-data-ingestion-test\n",
            "clean/00_SAP_P30/0004\n",
            "['clean/00_SAP_P30/0004/v1/', 'clean/00_SAP_P30/0004/v2/', 'clean/00_SAP_P30/0004/v3/']\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0004/v1/\n",
            "Size : ['552271279', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0004/v1']\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0004/v2/\n",
            "Size : ['84698956', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0004/v2']\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0004/v3/\n",
            "Size : ['233904036', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0004/v3']\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0006/v2/\n",
            "swarovski-advanced-analytics-data-ingestion-test\n",
            "clean/00_SAP_P30/0006\n",
            "['clean/00_SAP_P30/0006/v0/', 'clean/00_SAP_P30/0006/v1/', 'clean/00_SAP_P30/0006/v2/']\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0006/v0/\n",
            "Size : ['160705', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0006/v0']\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0006/v1/\n",
            "Size : ['20401428471', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0006/v1']\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0006/v2/\n",
            "Size : ['16806465304', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0006/v2']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path_pipeline_code_distinct_versions_check(df_9898.loc[i,'landing_zone_path'])\n",
        "df_9898['file_path_pipeline_code_distinct_versions_sizes']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwOvRtNJfZVW",
        "outputId": "c6884636-42ce-4a23-8c96-1ef893b6fd60"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0006/v0/\n",
            "Size : ['160705', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0006/v0']\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0006/v1/\n",
            "Size : ['20401428471', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0006/v1']\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0006/v2/\n",
            "Size : ['16806465304', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0006/v2']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [['233904036', 'gs://swarovski-advanced-analyt...\n",
              "1    [['16806465304', 'gs://swarovski-advanced-anal...\n",
              "Name: file_path_pipeline_code_distinct_versions_sizes, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bucket = df_9898.loc[i,'landing_zone_path'].split('/')[2]\n",
        "prefix  = df_9898.loc[i,'landing_zone_path'].split(bucket)[1][1:-4]\n",
        "\n",
        "#build a list containing path and corresponding sizes\n",
        "\n",
        "versions_sizes = list()\n",
        "\n",
        "for directory in list_directories(bucket,prefix):\n",
        "\n",
        "  path = \"gs://\"+ bucket + \"/\" +  directory \n",
        "  print(path)\n",
        "  size = os.popen(\"gsutil du -s \" + str(path)).read().split()\n",
        "  print(\"Size : \" + str(size))\n",
        "\n",
        "  versions_sizes.append(size)\n",
        "\n",
        "print(str(versions_sizes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJOHzV49dWNh",
        "outputId": "e92f1409-5f9a-47dd-cfc2-72648d324e23"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0006/v0/\n",
            "Size : ['160705', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0006/v0']\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0006/v1/\n",
            "Size : ['20401428471', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0006/v1']\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0006/v2/\n",
            "Size : ['16806465304', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0006/v2']\n",
            "[['160705', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0006/v0'], ['20401428471', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0006/v1'], ['16806465304', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0006/v2']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bucket = df_9898.loc[i,'landing_zone_path'].split('/')[2]\n",
        "prefix  = df_9898.loc[i,'landing_zone_path'].split(bucket)[1][1:-1]\n",
        "\n",
        "for directory in list_directories(bucket,prefix):\n",
        "\n",
        "  path = \"gs://\"+ bucket + \"/\" +  directory \n",
        "  print(path)\n",
        "  size = os.popen(\"gsutil du -s \" + str(path)).read().split()\n",
        "  print(\"Size : \" + str(size))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcXbCRB3PZ2m",
        "outputId": "e35d82c9-4293-42e1-917f-1e73663eaa20"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0006/v2/2018/\n",
            "Size : ['68821689', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0006/v2/2018']\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0006/v2/2019/\n",
            "Size : ['54734759', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0006/v2/2019']\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0006/v2/2020/\n",
            "Size : ['5349025804', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0006/v2/2020']\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0006/v2/2021/\n",
            "Size : ['6028383092', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0006/v2/2021']\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0006/v2/2022/\n",
            "Size : ['5305499960', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0006/v2/2022']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_id = 'user__rubini_riccardo'\n",
        "\n",
        "dataset_ref = client.dataset(dataset_id)\n",
        "job_config = bigquery.LoadJobConfig()\n",
        "job_config.autodetect = False\n",
        "job_config.write_disposition = \"WRITE_TRUNCATE\"\n",
        "#job_config.source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n",
        "load_job = client.load_table_from_dataframe(df_9898, dataset_ref.table(\"0001_dq_test\"), job_config=job_config, location=\"EU\")    # API request\n",
        "print(\"Starting job {}\".format(load_job))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-W-E_wSlaqHG",
        "outputId": "f2611381-b08f-4423-89f9-4f8160a8ee79"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/google/cloud/bigquery/_pandas_helpers.py:552: UserWarning: Pyarrow could not determine the type of columns: bucket_created, file_path_pipeline_code_checked_version, file_number_in_landing_zone_path, file_size_in_landing_zone_path, file_oldest_creation_datetime, file_newest_creation_datetime, file_name_policy_respected, file_name_pipeline_code_info, file_name_pipeline_name_info, file_name_pipeline_windowframe_info, file_name_pipeline_history_min_info, file_name_pipeline_history_max_info, files_period_covered, files_period_missing_between_min_max, warning_flag_status.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting job LoadJob<project=advanced-analytics-278408, location=EU, id=dc88373d-b51e-4899-acbc-dd96180e57ca>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IV_F9p_GfrKy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}