{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RiccardoRubini93/SWA_DQ/blob/main/Upload_data_to_BQ_latest_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "76kDLASjEmSv"
      },
      "outputs": [],
      "source": [
        "from google.api_core import page_iterator\n",
        "from google.cloud import storage\n",
        "import os\n",
        "import pandas as pd \n",
        "from google.cloud import bigquery\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "print('Authenticated')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsSPjTu_Z8R7",
        "outputId": "0394804a-a940-4beb-a5db-3cfcb99550b1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authenticated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_datetime_to_string(data):\n",
        "  return data.strftime('%Y-%d-%m | %H:%M:%S')"
      ],
      "metadata": {
        "id": "ZTYcv0jWrOl-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#next step is to add the extra columns\n",
        "\n",
        "def bucket_metadata(bucket_name,project_id):\n",
        "    \"\"\"Prints out a bucket's metadata.\"\"\"\n",
        "  \n",
        "    storage_client = storage.Client(project_id)\n",
        "    bucket = storage_client.get_bucket(bucket_name)\n",
        "\n",
        "    #print(f\"ID: {bucket.id}\")\n",
        "    #print(f\"Name: {bucket.name}\")\n",
        "    #print(f\"Storage Class: {bucket.storage_class}\")\n",
        "    #print(f\"Location: {bucket.location}\")\n",
        "    #print(f\"Location Type: {bucket.location_type}\")\n",
        "\n",
        "    data                  = {}\n",
        "    data['bucket_id']     = bucket.id\n",
        "    data['bucket_name']          = bucket.name\n",
        "    data['bucket_storage_class'] = bucket.storage_class\n",
        "    data['bucket_location']      = bucket.location\n",
        "    data['bucket_location_type'] = bucket.location_type\n",
        "    #json_data = json.dumps(data)\n",
        "\n",
        "    return data\n",
        "\n",
        "def file_path_policy_respected_check(file_path):\n",
        "    #function that checks whether the path policy of a certain bucket is respected or not\n",
        "    #each path should be in the following format \n",
        "    path_chuncks = file_path.split('/')[2:-1]\n",
        "    print(path_chuncks)\n",
        "    if len(path_chuncks) == 5 : policy_respected = True\n",
        "    else : policy_respected = False\n",
        "\n",
        "    return policy_respected\n",
        "\n",
        "def _item_to_value(iterator, item):\n",
        "    return item\n",
        "\n",
        "def list_directories(bucket_name, prefix):\n",
        "\n",
        "    if prefix and not prefix.endswith('/'):\n",
        "        prefix += '/'\n",
        "\n",
        "    extra_params = {\n",
        "        \"projection\": \"noAcl\",\n",
        "        \"prefix\": prefix,\n",
        "        \"delimiter\": '/'\n",
        "    }\n",
        "\n",
        "    gcs = storage.Client()\n",
        "\n",
        "    path = \"/b/\" + bucket_name + \"/o\"\n",
        "\n",
        "    iterator = page_iterator.HTTPIterator(\n",
        "        client=gcs,\n",
        "        api_request=gcs._connection.api_request,\n",
        "        path=path,\n",
        "        items_key='prefixes',\n",
        "        item_to_value=_item_to_value,\n",
        "        extra_params=extra_params,\n",
        "    )\n",
        "\n",
        "    return [x for x in iterator]"
      ],
      "metadata": {
        "id": "WuemepCphvDc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#definitions of helpers functions\n",
        "\n",
        "def file_path_pipeline_code_distinct_versions_check(input_path):\n",
        "\n",
        "  bucket = input_path.split('/')[2]\n",
        "  prefix  = input_path.split(bucket)[1][1:-4]\n",
        "\n",
        "  #build a list containing path and corresponding sizes\n",
        "\n",
        "  versions_sizes = []\n",
        "\n",
        "  for directory in list_directories(bucket,prefix):\n",
        "\n",
        "    path = \"gs://\"+ bucket + \"/\" +  directory \n",
        "    print(path)\n",
        "    size = os.popen(\"gsutil du -s \" + str(path)).read().split()\n",
        "    print(\"Size : \" + str(size))\n",
        "\n",
        "    versions_sizes.append(size)\n",
        "  return versions_sizes\n",
        "\n",
        "def get_file_number_in_landing_zone_path(input_path):\n",
        "\n",
        "    project_id = 'advanced-analytics-278408'\n",
        "    client = storage.Client(project_id)\n",
        "\n",
        "    bucket = input_path.split('/')[2]\n",
        "    prefix  = input_path.split(bucket)[1][1:-1]\n",
        "\n",
        "    count = 0\n",
        "\n",
        "    for blob in client.list_blobs(bucket,prefix=prefix):\n",
        "      count +=1\n",
        "\n",
        "    print(\"Landing zone path : \" + str(input_path) )\n",
        "    print(\"File number : \" + str(count))\n",
        "\n",
        "    return count\n",
        "\n",
        "def get_file_size_in_landing_zone_path(input_path):\n",
        "\n",
        "    bucket = input_path.split('/')[2]\n",
        "    print(bucket)\n",
        "    prefix  = input_path.split(bucket)[1][1:-4]\n",
        "    print(prefix)\n",
        "\n",
        "    # corresponding sizes\n",
        "\n",
        "    path = \"gs://\"+ bucket + \"/\" +  prefix \n",
        "    print(path)\n",
        "    size = os.popen(\"gsutil du -s \" + str(path)).read().split()[0]\n",
        "    print(\"Size : \" + str(size))\n",
        "  \n",
        "    return round(int(size) / 1024 ** 3, 2)\n",
        "\n",
        "def get_oldest_and_newest_file(input_path,project_id):\n",
        "\n",
        "  client = storage.Client(project_id)\n",
        "\n",
        "  bucket_name = input_path.split('/')[2]\n",
        "  prefix =      input_path.split(bucket)[1][1:-1]\n",
        "\n",
        "  #generate a list of blobs\n",
        "\n",
        "  blobs = [(blob, blob.updated) for blob in client.list_blobs(bucket_name,prefix = prefix,)]\n",
        "\n",
        "  #sort blobs by update date\n",
        "  newest  = sorted(blobs, key=lambda tup: tup[1])[-1] #ok funziona\n",
        "  oldest  = sorted(blobs, key=lambda tup: tup[1])[0]\n",
        "\n",
        "  return oldest,newest\n",
        "\n",
        "def is_file_policy_respected(input_path,project_id):\n",
        "\n",
        "  client = storage.Client(project_id)\n",
        "\n",
        "  name_is_conformal = True\n",
        "\n",
        "  print(\"Check that the policy of the file is respected\")\n",
        "  print(\"Bucket : \" + str(bucket))\n",
        "  print(\"Prefix : \" + str(prefix))\n",
        "\n",
        "  for blob in client.list_blobs(bucket,prefix=prefix):\n",
        "\n",
        "    if len((blob.name.split('/')[-1]).split('__')) != 4 : \n",
        "      print(\"Found not conformal file\")\n",
        "      print(blob.name)\n",
        "      name_is_conformal = False\n",
        "      break\n",
        "  \n",
        "  return name_is_conformal\n",
        "\n",
        "def extract_file_info(input_path,project_id):\n",
        "\n",
        "  client = storage.Client(project_id)\n",
        "\n",
        "  bucket_name = input_path.split('/')[2]\n",
        "  prefix =      input_path.split(bucket)[1][1:-1]\n",
        "\n",
        "  out = {}\n",
        "\n",
        "  cnt = 0\n",
        "\n",
        "  for blob in client.list_blobs(bucket,prefix=prefix):\n",
        "    \n",
        "    out['file_name_pipeline_code_info'] = blob.name.split('/')[2]\n",
        "    out['file_name_pipeline_name_info'] = blob.name.split('/')[1]\n",
        "    try: out['file_name_pipeline_windowframe_info'] = blob.name.split('/')[5].split('__')[2]\n",
        "    except: out['file_name_pipeline_windowframe_info'] = 'Missing File'\n",
        "\n",
        "    if cnt > 0 : break\n",
        "\n",
        "    cnt = +1\n",
        "\n",
        "  return out"
      ],
      "metadata": {
        "id": "XwD7H4xvQxVd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#generate full list of days for the period covered \n",
        "\n",
        "from datetime import datetime, date, timedelta\n",
        "\n",
        "def generate_list(period_covered,load_time_frame):\n",
        "\n",
        "  year_s = int(period_covered[0][0:4])\n",
        "  year_e = int(period_covered[1][0:4])\n",
        "\n",
        "  month_s = int(period_covered[0][4:6])\n",
        "  month_e = int(period_covered[1][4:6])\n",
        "\n",
        "  if load_time_frame == 'dd':\n",
        "\n",
        "    day_s = int(period_covered[0][6:8])\n",
        "    day_e = int(period_covered[1][6:8])\n",
        "    freq = 'd'\n",
        "\n",
        "  elif load_time_frame == 'mm':\n",
        "\n",
        "    day_s =  1\n",
        "    day_e =  1\n",
        "    freq = 'm'\n",
        "\n",
        "  start = date(year_s, month_s, day_s)  \n",
        "  end   = date(year_e, month_e, day_e) \n",
        "  \n",
        "  time_list = []\n",
        "\n",
        "  if load_time_frame == 'mm':\n",
        "    \n",
        "    for item in pd.date_range(start,end,freq=freq).strftime('%Y%m%d').to_list():\n",
        "\n",
        "      time_list.append(item[0:6])\n",
        "  \n",
        "  elif load_time_frame == 'dd':\n",
        "\n",
        "    time_list = pd.date_range(start,end,freq=freq).strftime('%Y%m%d').to_list() \n",
        "\n",
        "  \n",
        "  return time_list\n",
        "\n",
        "#select the oldest and newest file -> from filename\n",
        "def get_file_name_pipeline_history_min_and_max(project_id,file_path):\n",
        "\n",
        "  client = storage.Client(project_id)\n",
        "\n",
        "  bucket_name = file_path.split('/')[2]\n",
        "  prefix      = file_path.split(bucket)[1][1:-1]\n",
        "\n",
        "  print(bucket_name)\n",
        "  print(prefix)\n",
        "\n",
        "  #create a list of date where we pick the oldest and the newest\n",
        "  blobs = [[blob.name.split('/')[-1].split('__')[-1]] for blob in client.list_blobs(\n",
        "  bucket_name,\n",
        "  prefix = prefix)]\n",
        "\n",
        "  #extract the max and min date\n",
        "\n",
        "  newest = sorted(blobs, key=lambda tup: tup[0])[-1] \n",
        "  oldest = sorted(blobs, key=lambda tup: tup[0])[0]\n",
        "\n",
        "  return oldest,newest,blobs\n",
        "\n",
        "def check_missing_days(list1,list2):\n",
        "  #function that checks if all days are present \n",
        "  #list1 contains the set that needs to be checked\n",
        "  #list2 contains the whole list\n",
        "\n",
        "  missings = []\n",
        "\n",
        "  for item in list2:\n",
        "\n",
        "    if item in list1:\n",
        "      pass\n",
        "    else:\n",
        "      missings.append(item)\n",
        "\n",
        "  return missings"
      ],
      "metadata": {
        "id": "PIAxVwr66JUb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import timezone\n",
        "\n",
        "def get_warning (path, freq_agg):\n",
        "  today = datetime.datetime.now(timezone.utc)\n",
        "\n",
        "  bucket_name = path.split('/')[2]\n",
        "  prefix  = path.split(bucket_name)[1][1:-1]\n",
        "\n",
        "  storage_client = storage.Client()\n",
        "  bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "  l = []\n",
        "  l2 = []\n",
        "\n",
        "  for directory in list_directories(bucket_name,prefix):\n",
        "    print(directory)\n",
        "    l.append(directory)\n",
        "\n",
        "  blob_last_directory=max(l)\n",
        "\n",
        "  files = bucket.list_blobs()    \n",
        "  fileList = [file.name for file in files if blob_last_directory in file.name]\n",
        "\n",
        "  for file in fileList:\n",
        "    blob = bucket.get_blob(file)\n",
        "    l2.append(blob.updated)\n",
        "    print(blob.updated)\n",
        "  \n",
        "  blob_last_mod=max(l2)\n",
        "\n",
        "  if freq_agg=='dd':\n",
        "    if (today - blob_last_mod).days < 1:\n",
        "      return 'green'\n",
        "    elif (today - blob_last_mod).days >= 1 and (today - blob_last_mod).days <= 3:\n",
        "      return 'yellow'\n",
        "    else:\n",
        "      return 'red'\n",
        "  else:\n",
        "    if (today - blob_last_mod).days < 30:\n",
        "      return 'green'\n",
        "    elif (today - blob_last_mod).days >= 30 and (today - blob_last_mod).days <= 45:\n",
        "      return 'yellow'\n",
        "    else:\n",
        "      return 'red'\n"
      ],
      "metadata": {
        "id": "TrGFDrkBfypg"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "project_id = 'advanced-analytics-278408'\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = '''\n",
        "SELECT\n",
        "*\n",
        "FROM `advanced-analytics-278408.data__1st_layer.9891__monitoring_information_and_params`\n",
        "WHERE is_to_be_monitored = true\n",
        "and landing_zone = 'GCS';\n",
        "'''\n",
        "\n",
        "df = client.query(query).to_dataframe()\n",
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQw9AbRLZ-Gh",
        "outputId": "b77a3490-54f4-4f85-a2a2-9bacca0578ad"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['project_id', 'dataset_id', 'table_id', 'complete_table_id',\n",
              "       'table_code', 'is_to_be_monitored', 'current_development_status',\n",
              "       'current_development_status_id', 'source_system_id',\n",
              "       'source_system_name', 'source_system_ownership_type', 'extraction_tool',\n",
              "       'extraction_script', 'extraction_is_scheduled',\n",
              "       'extraction_schedule_pattern', 'landing_zone', 'landing_zone_path',\n",
              "       'landing_zone_form', 'loading_tool', 'load_script', 'load_is_scheduled',\n",
              "       'load_schedule_pattern', 'load_form', 'is_dx_framework_applied',\n",
              "       'id_field', 'data_loop_field', 'data_loop_granularity',\n",
              "       'overall_documentation_URL', 'to_do_pending', 'notes'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[0,'My_new_col'] = 'new_inserted_value'"
      ],
      "metadata": {
        "id": "PdPEHGOxbCOz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_2 = '''\n",
        "SELECT\n",
        "*\n",
        "FROM `advanced-analytics-278408.user__rubini_riccardo.9898__pipelines_target_landing_zone_metadata`;\n",
        "'''\n",
        "\n",
        "df_9898 = client.query(query_2).to_dataframe()"
      ],
      "metadata": {
        "id": "PSv9XGMjcGU8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_9898.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-nkujuNcYfk",
        "outputId": "d2fd704c-82c6-4f23-ebd9-a393f20474f8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "analysis_date                                      object\n",
              "analysis_date_time                                 object\n",
              "project_id                                         object\n",
              "dataset_id                                         object\n",
              "table_id                                           object\n",
              "complete_table_id                                  object\n",
              "table_code                                         object\n",
              "is_to_be_monitored                                 object\n",
              "current_development_status                         object\n",
              "current_development_status_id                      object\n",
              "source_system_id                                   object\n",
              "source_system_name                                 object\n",
              "source_system_ownership_type                       object\n",
              "extraction_tool                                    object\n",
              "extraction_script                                  object\n",
              "extraction_is_scheduled                            object\n",
              "extraction_schedule_pattern                        object\n",
              "landing_zone                                       object\n",
              "landing_zone_path                                  object\n",
              "landing_zone_form                                  object\n",
              "loading_tool                                       object\n",
              "load_script                                        object\n",
              "load_is_scheduled                                  object\n",
              "load_schedule_pattern                              object\n",
              "load_form                                          object\n",
              "bucket_id                                          object\n",
              "bucket_name                                        object\n",
              "bucket_created                                     object\n",
              "bucket_location                                    object\n",
              "bucket_location_type                               object\n",
              "bucket_storage_class                               object\n",
              "file_path_policy_respected                         object\n",
              "file_path_source_system                            object\n",
              "file_path_pipeline_code                            object\n",
              "file_path_pipeline_code_distinct_versions          object\n",
              "file_path_pipeline_code_distinct_versions_sizes    object\n",
              "file_path_pipeline_code_checked_version            object\n",
              "file_number_in_landing_zone_path                   object\n",
              "file_size_in_landing_zone_path                     object\n",
              "file_oldest_creation_datetime                      object\n",
              "file_newest_creation_datetime                      object\n",
              "file_name_policy_respected                         object\n",
              "file_name_pipeline_code_info                       object\n",
              "file_name_pipeline_name_info                       object\n",
              "file_name_pipeline_windowframe_info                object\n",
              "file_name_pipeline_history_min_info                object\n",
              "file_name_pipeline_history_max_info                object\n",
              "files_period_covered                               object\n",
              "files_period_missing_between_min_max               object\n",
              "warning_flag_status                                object\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "#fill the first N column of df_9898 with the first N columns of 9891__monitoring_information_and_params\n",
        "#define a list of columns to fill \n",
        "\n",
        "columns_list = [\n",
        "      'project_id', 'dataset_id', 'table_id', 'complete_table_id',\n",
        "       'table_code', 'is_to_be_monitored', 'current_development_status',\n",
        "       'current_development_status_id', 'source_system_id',\n",
        "       'source_system_name', 'source_system_ownership_type', 'extraction_tool',\n",
        "       'extraction_script', 'extraction_is_scheduled',\n",
        "       'extraction_schedule_pattern', 'landing_zone', 'landing_zone_path',\n",
        "       'landing_zone_form', 'loading_tool', 'load_script', 'load_is_scheduled',\n",
        "       'load_schedule_pattern', 'load_form'\n",
        "]\n",
        "\n",
        "\n",
        "#loop over the columns to fill \n",
        "\n",
        "for i in range(3):\n",
        "\n",
        "  #print the filepath analised\n",
        "  print(\"Computing row : \" + str(i) + \" of \" + str(df.shape[0]))\n",
        "  print(df.loc[i,'landing_zone_path'])\n",
        "\n",
        "  #fill the date\n",
        "\n",
        "  df_9898.loc[i,'analysis_date'] = datetime.datetime.now().strftime('%Y-%m-%d')\n",
        "  df_9898.loc[i,'analysis_date_time'] = datetime.datetime.now()\n",
        "\n",
        "  #add columns from the 9891\n",
        "  for col in columns_list:\n",
        "\n",
        "    df_9898.loc[i,col] = df.loc[i,col]\n",
        "\n",
        "  bucket = df.loc[i,'landing_zone_path'].split('/')[2]\n",
        "  bucket_meta = bucket_metadata(bucket,project_id)\n",
        "  \n",
        "  #add bucket metadata\n",
        "  for key in bucket_meta:\n",
        "    df_9898.loc[i,key] = bucket_meta[key] \n",
        "\n",
        "  #check that the policy and path info\n",
        "  df_9898.loc[i,'file_path_policy_respected'] = file_path_policy_respected_check(df_9898.loc[i,'landing_zone_path'])\n",
        "  df_9898.loc[i,'file_path_source_system'] = df_9898.loc[i,'landing_zone_path'].split('/')[5]\n",
        "  df_9898.loc[i,'file_path_pipeline_code'] = df_9898.loc[i,'landing_zone_path'].split('/')[4]\n",
        "\n",
        "  #extract bucket and prefix for a given landing_zone_path\n",
        "  bucket = df_9898.loc[i,'landing_zone_path'].split('/')[2]\n",
        "  prefix  = df_9898.loc[i,'landing_zone_path'].split(bucket)[1][1:-4]\n",
        "  #count the number of v1 v2 v3 \n",
        "  print(bucket)\n",
        "  print(prefix)\n",
        "  print(list_directories(bucket,prefix))\n",
        "\n",
        "  df_9898.loc[i,'file_path_pipeline_code_distinct_versions'] = len(list_directories(bucket,prefix))\n",
        "\n",
        "  #compute the size of each versioning of each folder\n",
        "\n",
        "  df_9898.loc[i,'file_path_pipeline_code_distinct_versions_sizes'] = str(file_path_pipeline_code_distinct_versions_check(df_9898.loc[i,'landing_zone_path']))\n",
        "  df_9898.loc[i,'file_path_pipeline_code_checked_version'] = 'TO BE INSERTED'\n",
        "\n",
        "  #compute the number of files in the landing zone path\n",
        "\n",
        "  df_9898.loc[i,'file_number_in_landing_zone_path'] = get_file_number_in_landing_zone_path(df_9898.loc[i,'landing_zone_path'],)\n",
        " \n",
        "  df_9898.loc[i,'file_size_in_landing_zone_path'] = get_file_size_in_landing_zone_path(df_9898.loc[i,'landing_zone_path'])\n",
        "\n",
        "  #get oldest and most recent file in folder\n",
        "\n",
        "  new = get_oldest_and_newest_file(df_9898.loc[i,'landing_zone_path'],project_id)[0][1]\n",
        "  old = get_oldest_and_newest_file(df_9898.loc[i,'landing_zone_path'],project_id)[1][1]\n",
        "\n",
        "  df_9898.loc[i,'file_oldest_creation_datetime'] = old\n",
        "  df_9898.loc[i,'file_newest_creation_datetime'] = new\n",
        "\n",
        "  #check policy of the file\n",
        "  df_9898.loc[i,'file_name_policy_respected']  = is_file_policy_respected(df_9898.loc[i,'landing_zone_path'],project_id)\n",
        "\n",
        "  #extract file infos\n",
        "\n",
        "  files_info = extract_file_info(df_9898.loc[i,'landing_zone_path'],project_id)\n",
        "\n",
        "  df_9898.loc[i,'file_name_pipeline_code_info']        =  files_info['file_name_pipeline_code_info']\n",
        "  df_9898.loc[i,'file_name_pipeline_name_info']        =  files_info['file_name_pipeline_name_info']\n",
        "  df_9898.loc[i,'file_name_pipeline_windowframe_info'] =  files_info['file_name_pipeline_windowframe_info']\n",
        "\n",
        "  #get first and latest date from filename\n",
        "\n",
        "  old,new,list_day = get_file_name_pipeline_history_min_and_max(project_id,df_9898.loc[i,'landing_zone_path'])\n",
        "  period_covered = [old[0],new[0]]\n",
        "\n",
        "  df_9898.loc[i,'file_name_pipeline_history_min_info'] = old[0]\n",
        "  df_9898.loc[i,'file_name_pipeline_history_max_info'] = new[0]\n",
        "  df_9898.loc[i,'files_period_covered'] = str(period_covered)\n",
        "\n",
        "  #get the complete list of days\n",
        "  try : \n",
        "    complete_list_days = generate_list(period_covered,df_9898.loc[i,'file_name_pipeline_windowframe_info'])\n",
        "    missings = check_missing_days(np.asarray(list_day)[:,0],complete_list_days)\n",
        "\n",
        "    if len(missings) == 0 : df_9898.loc[i,'files_period_missing_between_min_max'] = 'No missing days'\n",
        "    else: df_9898.loc[i,'files_period_missing_between_min_max'] = str(missings)\n",
        "  except:\n",
        "    print(\"Missing Files, step skipped\")\n",
        "  \n",
        "  #get_warning_status\n",
        "  df_9898.loc[i,'warning_flag_status'] = get_warning(df_9898.loc[i,'landing_zone_path'],df_9898.loc[i,'file_name_pipeline_windowframe_info'])\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQJmdc9YckAG",
        "outputId": "b0c49bff-8754-472b-927c-2669d3bf9817"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing row : 0 of 48\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0004/v3/\n",
            "['swarovski-advanced-analytics-data-ingestion-test', 'clean', '00_SAP_P30', '0004', 'v3']\n",
            "swarovski-advanced-analytics-data-ingestion-test\n",
            "clean/00_SAP_P30/0004\n",
            "['clean/00_SAP_P30/0004/v1/', 'clean/00_SAP_P30/0004/v2/', 'clean/00_SAP_P30/0004/v3/']\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0004/v1/\n",
            "Size : ['552271279', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0004/v1']\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0004/v2/\n",
            "Size : ['84698956', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0004/v2']\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0004/v3/\n",
            "Size : ['235949926', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0004/v3']\n",
            "Landing zone path : gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0004/v3/\n",
            "File number : 4018\n",
            "swarovski-advanced-analytics-data-ingestion-test\n",
            "clean/00_SAP_P30/0004\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0004\n",
            "Size : 872920161\n",
            "Check that the policy of the file is respected\n",
            "Bucket : swarovski-advanced-analytics-data-ingestion-test\n",
            "Prefix : clean/00_SAP_P30/0004\n",
            "swarovski-advanced-analytics-data-ingestion-test\n",
            "clean/00_SAP_P30/0004/v3\n",
            "Missing Files, step skipped\n",
            "Computing row : 1 of 48\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0006/v2/\n",
            "['swarovski-advanced-analytics-data-ingestion-test', 'clean', '00_SAP_P30', '0006', 'v2']\n",
            "swarovski-advanced-analytics-data-ingestion-test\n",
            "clean/00_SAP_P30/0006\n",
            "['clean/00_SAP_P30/0006/v0/', 'clean/00_SAP_P30/0006/v1/', 'clean/00_SAP_P30/0006/v2/']\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0006/v0/\n",
            "Size : ['160705', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0006/v0']\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0006/v1/\n",
            "Size : ['20401428471', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0006/v1']\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0006/v2/\n",
            "Size : ['17528196082', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0006/v2']\n",
            "Landing zone path : gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0006/v2/\n",
            "File number : 1824\n",
            "swarovski-advanced-analytics-data-ingestion-test\n",
            "clean/00_SAP_P30/0006\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0006\n",
            "Size : 37929785258\n",
            "Check that the policy of the file is respected\n",
            "Bucket : swarovski-advanced-analytics-data-ingestion-test\n",
            "Prefix : clean/00_SAP_P30/0006\n",
            "swarovski-advanced-analytics-data-ingestion-test\n",
            "clean/00_SAP_P30/0006/v2\n",
            "Missing Files, step skipped\n",
            "Computing row : 2 of 48\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0019/v1/\n",
            "['swarovski-advanced-analytics-data-ingestion-test', 'clean', '00_SAP_P30', '0019', 'v1']\n",
            "swarovski-advanced-analytics-data-ingestion-test\n",
            "clean/00_SAP_P30/0019\n",
            "['clean/00_SAP_P30/0019/v1/']\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0019/v1/\n",
            "Size : ['28681685275', 'gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0019/v1']\n",
            "Landing zone path : gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0019/v1/\n",
            "File number : 1824\n",
            "swarovski-advanced-analytics-data-ingestion-test\n",
            "clean/00_SAP_P30/0019\n",
            "gs://swarovski-advanced-analytics-data-ingestion-test/clean/00_SAP_P30/0019\n",
            "Size : 28681685275\n",
            "Check that the policy of the file is respected\n",
            "Bucket : swarovski-advanced-analytics-data-ingestion-test\n",
            "Prefix : clean/00_SAP_P30/0019\n",
            "swarovski-advanced-analytics-data-ingestion-test\n",
            "clean/00_SAP_P30/0019/v1\n",
            "Missing Files, step skipped\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#select the oldest and newest file -> from filename\n",
        "def get_file_name_pipeline_history_min_and_max(project_id,file_path):\n",
        "\n",
        "  client = storage.Client(project_id)\n",
        "\n",
        "  bucket_name = file_path.split('/')[2]\n",
        "  prefix      = file_path.split(bucket)[1][1:-1]\n",
        "\n",
        "  print(bucket_name)\n",
        "  print(prefix)\n",
        "\n",
        "  #create a list of date where we pick the oldest and the newest\n",
        "  blobs = [[blob.name.split('/')[-1].split('__')[-1]] for blob in client.list_blobs(\n",
        "  bucket_name,\n",
        "  prefix = prefix)]\n",
        "\n",
        "  #extract the max and min date\n",
        "\n",
        "  newest = sorted(blobs, key=lambda tup: tup[0])[-1] \n",
        "  oldest = sorted(blobs, key=lambda tup: tup[0])[0]\n",
        "\n",
        "  return oldest,newest,blobs\n",
        "\n",
        "#get_file_name_pipeline_history_min_and_max(project_id,df_9898.loc[0,'landing_zone_path']) #test daily load\n",
        "\n",
        "old,new,list_day = get_file_name_pipeline_history_min_and_max(project_id,df_9898.loc[4,'landing_zone_path'])  #test monthly load\n",
        "\n",
        "#compute file period covered \n",
        "\n",
        "period_covered = [old[0],new[0]]\n",
        "period_covered\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "mwOvRtNJfZVW",
        "outputId": "17b9b3f3-b936-4aed-b206-0451358acca6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 4",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-fd4d432d7e05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#get_file_name_pipeline_history_min_and_max(project_id,df_9898.loc[0,'landing_zone_path']) #test daily load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlist_day\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_file_name_pipeline_history_min_and_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_9898\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'landing_zone_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#test monthly load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m#compute file period covered\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    923\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0msuppress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_takeable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    926\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0;31m# we by definition only have the 0th axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msuppress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIndexingError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m         \u001b[0;31m# no multi-index, so validate all of the indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_lowerdim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m    836\u001b[0m                 \u001b[0;31m# We don't need to check for tuples here because those are\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m                 \u001b[0;31m#  caught by the _is_nested_tuple_indexer check above.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 838\u001b[0;31m                 \u001b[0msection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m                 \u001b[0;31m# We should never have a scalar section here, because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;31m# fall thru to straight lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1164\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_slice_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice_obj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_label\u001b[0;34m(self, label, axis)\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0;31m# GH#5667 this will fail if the label is not present in the axis.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1113\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_handle_lowerdim_multi_index_axis0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mxs\u001b[0;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[1;32m   3774\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Expected label or tuple of labels, got {key}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3775\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3776\u001b[0;31m             \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3778\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 4"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#generate list of days from the period covered\n",
        "\n",
        "from datetime import datetime, date, timedelta\n",
        "\n",
        "def generate_list(period_covered,load_time_frame):\n",
        "\n",
        "  year_s = int(period_covered[0][0:4])\n",
        "  year_e = int(period_covered[1][0:4])\n",
        "\n",
        "  month_s = int(period_covered[0][4:6])\n",
        "  month_e = int(period_covered[1][4:6])\n",
        "\n",
        "  if load_time_frame == 'dd':\n",
        "\n",
        "    day_s = int(period_covered[0][6:8])\n",
        "    day_e = int(period_covered[1][6:8])\n",
        "    freq = 'd'\n",
        "\n",
        "  elif load_time_frame == 'mm':\n",
        "\n",
        "    day_s =  1\n",
        "    day_e =  1\n",
        "    freq = 'm'\n",
        "\n",
        "  start = date(year_s, month_s, day_s)  \n",
        "  end   = date(year_e, month_e, day_e) \n",
        "  \n",
        "  time_list = []\n",
        "\n",
        "  if load_time_frame == 'mm':\n",
        "    \n",
        "    for item in pd.date_range(start,end,freq=freq).strftime('%Y%m%d').to_list():\n",
        "\n",
        "      time_list.append(item[0:6])\n",
        "  \n",
        "  elif load_time_frame == 'dd':\n",
        "\n",
        "    time_list = pd.date_range(start,end,freq=freq).strftime('%Y%m%d').to_list() \n",
        "\n",
        "  \n",
        "  return time_list\n",
        "    \n",
        "\n",
        "complete_list = generate_list(period_covered,'mm')\n",
        "complete_list"
      ],
      "metadata": {
        "id": "hfcYazsyzQuS",
        "outputId": "f4795fa7-d761-4562-c787-ef3a52c873a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['201201',\n",
              " '201202',\n",
              " '201203',\n",
              " '201204',\n",
              " '201205',\n",
              " '201206',\n",
              " '201207',\n",
              " '201208',\n",
              " '201209',\n",
              " '201210',\n",
              " '201211',\n",
              " '201212',\n",
              " '201301',\n",
              " '201302',\n",
              " '201303',\n",
              " '201304',\n",
              " '201305',\n",
              " '201306',\n",
              " '201307',\n",
              " '201308',\n",
              " '201309',\n",
              " '201310',\n",
              " '201311',\n",
              " '201312',\n",
              " '201401',\n",
              " '201402',\n",
              " '201403',\n",
              " '201404',\n",
              " '201405',\n",
              " '201406',\n",
              " '201407',\n",
              " '201408',\n",
              " '201409',\n",
              " '201410',\n",
              " '201411',\n",
              " '201412',\n",
              " '201501',\n",
              " '201502',\n",
              " '201503',\n",
              " '201504',\n",
              " '201505',\n",
              " '201506',\n",
              " '201507',\n",
              " '201508',\n",
              " '201509',\n",
              " '201510',\n",
              " '201511',\n",
              " '201512',\n",
              " '201601',\n",
              " '201602',\n",
              " '201603',\n",
              " '201604',\n",
              " '201605',\n",
              " '201606',\n",
              " '201607',\n",
              " '201608',\n",
              " '201609',\n",
              " '201610',\n",
              " '201611',\n",
              " '201612',\n",
              " '201701',\n",
              " '201702',\n",
              " '201703',\n",
              " '201704',\n",
              " '201705',\n",
              " '201706',\n",
              " '201707',\n",
              " '201708',\n",
              " '201709',\n",
              " '201710',\n",
              " '201711',\n",
              " '201712',\n",
              " '201801',\n",
              " '201802',\n",
              " '201803',\n",
              " '201804',\n",
              " '201805',\n",
              " '201806',\n",
              " '201807',\n",
              " '201808',\n",
              " '201809',\n",
              " '201810',\n",
              " '201811',\n",
              " '201812',\n",
              " '201901',\n",
              " '201902',\n",
              " '201903',\n",
              " '201904',\n",
              " '201905',\n",
              " '201906',\n",
              " '201907',\n",
              " '201908',\n",
              " '201909',\n",
              " '201910',\n",
              " '201911',\n",
              " '201912',\n",
              " '202001',\n",
              " '202002',\n",
              " '202003',\n",
              " '202004',\n",
              " '202005',\n",
              " '202006',\n",
              " '202007',\n",
              " '202008',\n",
              " '202009',\n",
              " '202010',\n",
              " '202011',\n",
              " '202012',\n",
              " '202101',\n",
              " '202102',\n",
              " '202103',\n",
              " '202104',\n",
              " '202105',\n",
              " '202106',\n",
              " '202107',\n",
              " '202108',\n",
              " '202109',\n",
              " '202110',\n",
              " '202111',\n",
              " '202112',\n",
              " '202201',\n",
              " '202202',\n",
              " '202203',\n",
              " '202204',\n",
              " '202205',\n",
              " '202206',\n",
              " '202207',\n",
              " '202208',\n",
              " '202209',\n",
              " '202210',\n",
              " '202211']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check lists \n",
        "def check_missing_days(list1,list2):\n",
        "  #function that checks if all days are present \n",
        "  #list1 contains the set that needs to be checked\n",
        "  #list2 contains the whole list\n",
        "\n",
        "  missings = []\n",
        "\n",
        "  for item in list2:\n",
        "\n",
        "    if item in list1:\n",
        "      pass\n",
        "    else:\n",
        "      missings.append(item)\n",
        "\n",
        "  return missings\n",
        "\n"
      ],
      "metadata": {
        "id": "Q2fGZyEjAPdJ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "for i in range(len(complete_list)):\n",
        "  print(complete_list[i],list_day[i][0])"
      ],
      "metadata": {
        "id": "u0UjN7tXCD78",
        "outputId": "e127665c-dcf1-4d76-b354-55b6c0f15f85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "201201 201201\n",
            "201202 201202\n",
            "201203 201203\n",
            "201204 201204\n",
            "201205 201205\n",
            "201206 201206\n",
            "201207 201207\n",
            "201208 201208\n",
            "201209 201209\n",
            "201210 201210\n",
            "201211 201211\n",
            "201212 201212\n",
            "201301 201301\n",
            "201302 201302\n",
            "201303 201303\n",
            "201304 201304\n",
            "201305 201305\n",
            "201306 201306\n",
            "201307 201307\n",
            "201308 201308\n",
            "201309 201309\n",
            "201310 201310\n",
            "201311 201311\n",
            "201312 201312\n",
            "201401 201401\n",
            "201402 201402\n",
            "201403 201403\n",
            "201404 201404\n",
            "201405 201405\n",
            "201406 201406\n",
            "201407 201407\n",
            "201408 201408\n",
            "201409 201409\n",
            "201410 201410\n",
            "201411 201411\n",
            "201412 201412\n",
            "201501 201501\n",
            "201502 201502\n",
            "201503 201503\n",
            "201504 201504\n",
            "201505 201505\n",
            "201506 201506\n",
            "201507 201507\n",
            "201508 201508\n",
            "201509 201509\n",
            "201510 201510\n",
            "201511 201511\n",
            "201512 201512\n",
            "201601 201601\n",
            "201602 201602\n",
            "201603 201603\n",
            "201604 201604\n",
            "201605 201605\n",
            "201606 201606\n",
            "201607 201607\n",
            "201608 201608\n",
            "201609 201609\n",
            "201610 201610\n",
            "201611 201611\n",
            "201612 201612\n",
            "201701 201701\n",
            "201702 201702\n",
            "201703 201703\n",
            "201704 201704\n",
            "201705 201705\n",
            "201706 201706\n",
            "201707 201707\n",
            "201708 201708\n",
            "201709 201709\n",
            "201710 201710\n",
            "201711 201711\n",
            "201712 201712\n",
            "201801 201801\n",
            "201802 201802\n",
            "201803 201803\n",
            "201804 201804\n",
            "201805 201805\n",
            "201806 201806\n",
            "201807 201807\n",
            "201808 201808\n",
            "201809 201809\n",
            "201810 201810\n",
            "201811 201811\n",
            "201812 201812\n",
            "201901 201901\n",
            "201902 201902\n",
            "201903 201903\n",
            "201904 201904\n",
            "201905 201905\n",
            "201906 201906\n",
            "201907 201907\n",
            "201908 201908\n",
            "201909 201909\n",
            "201910 201910\n",
            "201911 201911\n",
            "201912 201912\n",
            "202001 202001\n",
            "202002 202002\n",
            "202003 202003\n",
            "202004 202004\n",
            "202005 202005\n",
            "202006 202006\n",
            "202007 202007\n",
            "202008 202008\n",
            "202009 202009\n",
            "202010 202010\n",
            "202011 202011\n",
            "202012 202012\n",
            "202101 202101\n",
            "202102 202102\n",
            "202103 202103\n",
            "202104 202104\n",
            "202105 202105\n",
            "202106 202106\n",
            "202107 202107\n",
            "202108 202108\n",
            "202109 202109\n",
            "202110 202110\n",
            "202111 202111\n",
            "202112 202112\n",
            "202201 202201\n",
            "202202 202202\n",
            "202203 202203\n",
            "202204 202204\n",
            "202205 202205\n",
            "202206 202206\n",
            "202207 202207\n",
            "202208 202208\n",
            "202209 202209\n",
            "202210 202210\n",
            "202211 202211\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "missings = check_missing_days(np.asarray(list_day)[:,0],complete_list)\n",
        "\n",
        "if len(missings) == 0 : is_list_empty = True\n",
        "else: is_list_empty = False"
      ],
      "metadata": {
        "id": "PI92BSIdAoXU"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missings\n",
        "is_list_empty"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ur8Z51nlBKtZ",
        "outputId": "c4bb8f42-85c6-4c96-8f8b-8d1fc4280397"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_id = 'user__rubini_riccardo'\n",
        "\n",
        "dataset_ref = client.dataset(dataset_id)\n",
        "job_config = bigquery.LoadJobConfig()\n",
        "job_config.autodetect = False\n",
        "job_config.write_disposition = \"WRITE_TRUNCATE\"\n",
        "#job_config.source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n",
        "load_job = client.load_table_from_dataframe(df_9898, dataset_ref.table(\"9898__pipelines_target_landing_zone_metadata_v1\"), job_config=job_config, location=\"EU\")    # API request\n",
        "print(\"Starting job {}\".format(load_job))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-W-E_wSlaqHG",
        "outputId": "593f34c4-1097-4371-ef78-fa3d9f5c0277"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/google/cloud/bigquery/_pandas_helpers.py:571: UserWarning: Pyarrow could not determine the type of columns: bucket_created, files_period_missing_between_min_max.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting job LoadJob<project=advanced-analytics-278408, location=EU, id=46fa4a7e-c07b-450e-a7d9-5c9f4392e49d>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IV_F9p_GfrKy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}